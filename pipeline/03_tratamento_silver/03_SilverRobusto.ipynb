{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969437f1-be54-4df4-8a9c-32d999d4132b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771307934847}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>nome</th><th>idade</th><th>email</th><th>cidade</th><th>salario</th><th>ingestion_time</th></tr></thead><tbody><tr><td>1</td><td>Pessoa 0</td><td>26</td><td>user0@exemplo.com</td><td>null</td><td>2036.32</td><td>2026/02/19-18:05:36</td></tr><tr><td>2</td><td>Pessoa 1</td><td>68</td><td>user1@exemplo.com</td><td>Rio</td><td>3705.09</td><td>2026/02/19-18:05:36</td></tr><tr><td>3</td><td>Pessoa 2</td><td>67</td><td>user2@exemplo.com</td><td>São Paulo</td><td>5693.16</td><td>2026/02/19-18:05:36</td></tr><tr><td>4</td><td>Pessoa 3</td><td>38</td><td>user3@exemplo.com</td><td>Belo Horizonte</td><td>9376.51</td><td>2026/02/19-18:05:36</td></tr><tr><td>5</td><td>Pessoa 4</td><td>53</td><td>user4@exemplo.com</td><td>null</td><td>6392.52</td><td>2026/02/19-18:05:36</td></tr><tr><td>6</td><td>Pessoa 5</td><td>49</td><td>user5@exemplo.com</td><td>Rio</td><td>7172.05</td><td>2026/02/19-18:05:36</td></tr><tr><td>7</td><td>Pessoa 6</td><td>null</td><td>user6@exemplo.com</td><td>null</td><td>3802.53</td><td>2026/02/19-18:05:36</td></tr><tr><td>8</td><td>Pessoa 7</td><td>47</td><td>user7@exemplo.com</td><td>null</td><td>4748.2</td><td>2026/02/19-18:05:36</td></tr><tr><td>9</td><td>Pessoa 8</td><td>27</td><td>user8@exemplo.com</td><td>Belo Horizonte</td><td>4476.1</td><td>2026/02/19-18:05:36</td></tr><tr><td>10</td><td>Pessoa 9</td><td>61</td><td>user9@exemplo.com</td><td>null</td><td>9242.02</td><td>2026/02/19-18:05:36</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Pessoa 0",
         "26",
         "user0@exemplo.com",
         null,
         2036.32,
         "2026/02/19-18:05:36"
        ],
        [
         2,
         "Pessoa 1",
         "68",
         "user1@exemplo.com",
         "Rio",
         3705.09,
         "2026/02/19-18:05:36"
        ],
        [
         3,
         "Pessoa 2",
         "67",
         "user2@exemplo.com",
         "São Paulo",
         5693.16,
         "2026/02/19-18:05:36"
        ],
        [
         4,
         "Pessoa 3",
         "38",
         "user3@exemplo.com",
         "Belo Horizonte",
         9376.51,
         "2026/02/19-18:05:36"
        ],
        [
         5,
         "Pessoa 4",
         "53",
         "user4@exemplo.com",
         null,
         6392.52,
         "2026/02/19-18:05:36"
        ],
        [
         6,
         "Pessoa 5",
         "49",
         "user5@exemplo.com",
         "Rio",
         7172.05,
         "2026/02/19-18:05:36"
        ],
        [
         7,
         "Pessoa 6",
         null,
         "user6@exemplo.com",
         null,
         3802.53,
         "2026/02/19-18:05:36"
        ],
        [
         8,
         "Pessoa 7",
         "47",
         "user7@exemplo.com",
         null,
         4748.2,
         "2026/02/19-18:05:36"
        ],
        [
         9,
         "Pessoa 8",
         "27",
         "user8@exemplo.com",
         "Belo Horizonte",
         4476.1,
         "2026/02/19-18:05:36"
        ],
        [
         10,
         "Pessoa 9",
         "61",
         "user9@exemplo.com",
         null,
         9242.02,
         "2026/02/19-18:05:36"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "nome",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "idade",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "cidade",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salario",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ingestion_time",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver 2 recriada com schema do arquivo atual\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- \n",
    "# IMPORTS (TOPO)\n",
    "# -----------------------------\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, lower, coalesce, expr, regexp_replace, when, isnan, \n",
    "    to_timestamp, current_timestamp, lit, to_date\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType, FloatType, LongType, DecimalType\n",
    "\n",
    "# -----------------------------\n",
    "# 1 Inicializa Spark\n",
    "# -----------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Lê Silver 1\n",
    "# -----------------------------\n",
    "silver_1_table = \"saas_project.core.silver_data\"\n",
    "df = spark.table(silver_1_table)\n",
    "\n",
    "# -----------------------------\n",
    "# 3 Normaliza nomes das colunas\n",
    "# -----------------------------\n",
    "def normalizar_coluna(nome):\n",
    "    \"\"\"\n",
    "    Converte o nome da coluna para minúsculo e substitui caracteres especiais por underscore.\n",
    "    Mantém apenas letras, números e underscore.\n",
    "    \"\"\"\n",
    "    nome = nome.lower()\n",
    "    nome = re.sub(r'[^a-z0-9_]', '_', nome)\n",
    "    nome = re.sub(r'_+', '_', nome)\n",
    "    return nome.strip('_')\n",
    "\n",
    "df = df.toDF(*[normalizar_coluna(c) for c in df.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 4 Identifica tipos\n",
    "# -----------------------------\n",
    "string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "numeric_cols = [c for c, t in df.dtypes if t in [\"int\", \"bigint\", \"double\", \"float\", \"decimal\"]]\n",
    "timestamp_cols = [c for c, t in df.dtypes if t.startswith(\"timestamp\") or \"date\" in c.lower()]\n",
    "\n",
    "# -----------------------------\n",
    "# 5 Limpeza strings\n",
    "# -----------------------------\n",
    "valores_invalidos_ext = [\n",
    "    \"null\",\"na\",\"n/a\",\"nan\",\"-\",\"none\",\"undefined\",\"erro\",\"error\",\n",
    "    \"fail\",\"failed\",\"invalid\",\"invalido\",\"inválido\",\n",
    "    \"not applicable\",\"n.a\",\"nao aplicavel\",\"não aplicavel\",\n",
    "    \"missing\",\"missing_value\",\"unknown\",\"unk\",\"nullstr\",\n",
    "    \"??\",\"???\",\"@\",\"@@\",\"?\",\"nan\",\"ABC\",\"NaN\"\n",
    "]\n",
    "invalidos_lower = [v.lower().strip() for v in valores_invalidos_ext]\n",
    "\n",
    "# -----------------------------\n",
    "# 5.0 Limpeza de inválidos em TODAS as colunas (qualquer tipo)\n",
    "# -----------------------------\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        when(\n",
    "            lower(trim(col(c).cast(\"string\"))).isin(invalidos_lower),\n",
    "            None\n",
    "        ).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5.1 Limpeza de números inválidos (NaN, etc)\n",
    "# -----------------------------\n",
    "numeric_cols_all = [\n",
    "    field.name for field in df.schema.fields\n",
    "    if isinstance(field.dataType, (IntegerType, DoubleType, FloatType, LongType, DecimalType))\n",
    "]\n",
    "\n",
    "for c in numeric_cols_all:\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        when(isnan(col(c)), None).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 5.2 Limpeza avançada de strings\n",
    "# -----------------------------\n",
    "for c in string_cols:\n",
    "    # Remove tabs, quebras de linha e espaços extras\n",
    "    df = df.withColumn(c, trim(regexp_replace(col(c), r\"[\\t\\n\\r]\", \"\")))\n",
    "    \n",
    "    # Remove valores inválidos\n",
    "    df = df.withColumn(c, when(lower(trim(col(c))).isin(invalidos_lower), None).otherwise(col(c)))\n",
    "\n",
    "    # REGRA AJUSTADA – não apaga mais \"Pessoa 1\", \"Cliente 2\", etc\n",
    "    df = df.withColumn(\n",
    "        c,\n",
    "        when(\n",
    "            col(c).rlike(\"^[0-9a-zA-Z]+$\") &\n",
    "            col(c).rlike(\".*[0-9].*\") &\n",
    "            col(c).rlike(\".*[a-zA-Z].*\"),\n",
    "            None\n",
    "        ).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "    # Remove strings que só têm caracteres não alfanuméricos\n",
    "    df = df.withColumn(c, when(regexp_replace(col(c), r\"[a-z0-9]\", \"\") == col(c), None).otherwise(col(c)))\n",
    "    \n",
    "    # Remove strings vazias\n",
    "    df = df.withColumn(c, when(trim(col(c)) == \"\", None).otherwise(col(c)))\n",
    "\n",
    "# -----------------------------\n",
    "# 5.3 Observação importante:\n",
    "# Não alteramos maiúsculas/minúsculas do conteúdo original.\n",
    "# O texto será mantido conforme veio no arquivo:\n",
    "# - Primeira letra maiúscula se estiver maiúscula\n",
    "# - Minúsculas permanecem minúsculas (ex.: e-mails)\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# 6 Conversão números e tratamento de negativos\n",
    "# -----------------------------\n",
    "numeric_cols = [field.name for field in df.schema.fields \n",
    "                if isinstance(field.dataType, (IntegerType, DoubleType, FloatType, LongType, DecimalType))]\n",
    "\n",
    "for c in numeric_cols:\n",
    "    temp_col = c + \"_temp\"\n",
    "    df = df.withColumn(temp_col, col(c).cast(DoubleType()))\n",
    "    df = df.drop(c).withColumnRenamed(temp_col, c)\n",
    "\n",
    "# -----------------------------\n",
    "# 6.1 Ajusta coluna ID (se existir)\n",
    "# -----------------------------\n",
    "if \"id\" in df.columns:\n",
    "    # Converte para inteiro\n",
    "    df = df.withColumn(\"id\", col(\"id\").cast(IntegerType()))\n",
    "    \n",
    "    # Ordena crescente\n",
    "    df = df.orderBy(\"id\")\n",
    "    \n",
    "    # Move id para primeira coluna\n",
    "    other_cols = [c for c in df.columns if c != \"id\"]\n",
    "    df = df.select([\"id\"] + other_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 7 Datas\n",
    "# -----------------------------\n",
    "for c in timestamp_cols:\n",
    "    ts_col = c + \"_ts\"\n",
    "    df = df.withColumn(ts_col, lit(None).cast(\"timestamp\"))\n",
    "    df = df.withColumn(ts_col, coalesce(\n",
    "        col(ts_col),\n",
    "        when(col(c).rlike(r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$\"), to_timestamp(col(c), \"yyyy-MM-dd HH:mm:ss\")),\n",
    "        when(col(c).rlike(r\"^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}Z$\"), to_timestamp(col(c), \"dd/MM/yyyy HH:mm:ss\")),\n",
    "        when(col(c).rlike(r\"^\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}$\"), to_timestamp(col(c), \"dd-MM-yyyy HH:mm:ss\")),\n",
    "        when(col(c).rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"), to_timestamp(col(c), \"yyyy-MM-dd\")),\n",
    "        when(col(c).rlike(r\"^\\d{2}/\\d{2}/\\d{4}$\"), to_timestamp(col(c), \"dd/MM/yyyy\")),\n",
    "        when(col(c).rlike(r\"^\\d{2}-\\d{2}-\\d{4}$\"), to_timestamp(col(c), \"dd-MM-yyyy\"))\n",
    "    ))\n",
    "    df = df.drop(c).withColumnRenamed(ts_col, c)\n",
    "    df = df.withColumn(c, when(col(c) > current_timestamp(), None).otherwise(col(c)))\n",
    "\n",
    "# -----------------------------\n",
    "# 7.1 Corrige data_contratacao_date\n",
    "# -----------------------------\n",
    "if \"data_contratacao_date\" in df.columns:\n",
    "    df = df.withColumn(\n",
    "        \"data_contratacao_date\",\n",
    "        to_timestamp(col(\"data_contratacao_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSXXX\")\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"data_contratacao_date\",\n",
    "        to_date(col(\"data_contratacao_date\"))\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 7.2 Mantém ingestion_time como última coluna\n",
    "# -----------------------------\n",
    "if \"ingestion_time\" in df.columns:\n",
    "    cols = [c for c in df.columns if c != \"ingestion_time\"] + [\"ingestion_time\"]\n",
    "    df = df.select(*cols)\n",
    "\n",
    "# -----------------------------\n",
    "# 8 Mantém índice se existir\n",
    "# -----------------------------\n",
    "if \"idx\" in df.columns:\n",
    "    df = df.orderBy(\"idx\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8.1 Remove duplicados (ignorando ingestion_time)\n",
    "# -----------------------------\n",
    "if \"ingestion_time\" in df.columns:\n",
    "    cols_dedup = [c for c in df.columns if c != \"ingestion_time\"]\n",
    "    df = df.dropDuplicates(cols_dedup)\n",
    "else:\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "display(df.limit(10))\n",
    "\n",
    "# -----------------------------\n",
    "# 9 Salva Silver 2 (somente tabela Delta)\n",
    "# -----------------------------\n",
    "silver_2_table = \"saas_project.core.silver_2_data\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {silver_2_table}\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_2_table)\n",
    "print(\"Silver 2 recriada com schema do arquivo atual\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10 Salva Gold CSV (apenas se for Gold)\n",
    "# -----------------------------\n",
    "is_gold = False  # <- definir True só quando gerar CSV Gold\n",
    "\n",
    "if is_gold:\n",
    "    output_dir = \"/Volumes/saas_project/core/download/gold_data_csv/\"\n",
    "    final_csv_path = os.path.join(output_dir, \"gold_data.csv\")\n",
    "\n",
    "    df_gold_csv = df  # mantém todos os dados do Silver 2 tratados\n",
    "\n",
    "    temp_path = os.path.join(output_dir, \"temp_csv\")\n",
    "    df_gold_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_path)\n",
    "\n",
    "    for file_name in os.listdir(temp_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            shutil.move(os.path.join(temp_path, file_name), final_csv_path)\n",
    "            break\n",
    "\n",
    "    shutil.rmtree(temp_path)\n",
    "    print(f\"CSV Gold salvo em: {final_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_SilverRobusto",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}